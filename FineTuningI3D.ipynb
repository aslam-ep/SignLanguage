{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuningI3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVBJpRS_WRQE"
      },
      "source": [
        "**1. Installing the requirements**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "laQS4fsmWEnH",
        "outputId": "cd5b075a-0869-46d6-b855-89a125742496"
      },
      "source": [
        "# for mxnet\n",
        "!pip install --upgrade mxnet\n",
        "# for pytorch\n",
        "!pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip install --upgrade gluoncv\n",
        "\n",
        "!pip install mxnet-cu101"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/07/66174e78c12a3048db9039aaa09553e35035ef3a008ba3e0ed8d2aa3c47b/mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9MB 63kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.6.0+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (154.6MB)\n",
            "\u001b[K     |████████████████████████████████| 154.6MB 78kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.7.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (5.1MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cpu) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cpu) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0+cpu) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0+cpu which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.6.0+cpu torchvision-0.7.0+cpu\n",
            "Collecting gluoncv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/2b/f1cc4e7a0a654c00dae9d870d0d1f653e5760ad85297306b63e6be527dcf/gluoncv-0.10.1.post0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.3MB/s \n",
            "\u001b[?25hCollecting autogluon.core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/15/1f0f86152ffb389214b848baf9c44ac02e266dbc218a4273de78dc09316f/autogluon.core-0.1.0-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 54.3MB/s \n",
            "\u001b[?25hCollecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from gluoncv) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from gluoncv) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from gluoncv) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from gluoncv) (1.19.5)\n",
            "Collecting decord\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/5e/e2be6a3a3a46275059574d9c6a1d422aa6c7c3cbf6614939b8a3c3f8f2d5/decord-0.5.2-py3-none-manylinux2010_x86_64.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1MB 256kB/s \n",
            "\u001b[?25hCollecting yacs\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from gluoncv) (1.4.1)\n",
            "Collecting autocfg\n",
            "  Downloading https://files.pythonhosted.org/packages/95/f9/74e0a42cbc6d871c92288806e7812c7d2628c2a06557930dbab0a17438d2/autocfg-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from gluoncv) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from gluoncv) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from gluoncv) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (2.12.0)\n",
            "Collecting distributed>=2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b0/3454dc44239c526f9c9e4cf04f62823776b71f927db74302986d56e7a9a1/distributed-2021.4.0-py3-none-any.whl (684kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (1.3)\n",
            "Collecting paramiko>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 59.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (0.29.22)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/f2/52bf45246ad421cdd8c80a5c0f3e8f082e818d0467cde6aa8294258d1b4a/boto3-1.17.55-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 59.7MB/s \n",
            "\u001b[?25hCollecting ConfigSpace==0.4.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c3/3c21e8d82a639fd821f538e6d7f830b654a6ce1fe52644be1a67f323f707/ConfigSpace-0.4.18.tar.gz (950kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 50.6MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn<0.25,>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: dill==0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core->gluoncv) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx->gluoncv) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gluoncv) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gluoncv) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gluoncv) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gluoncv) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gluoncv) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gluoncv) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gluoncv) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gluoncv) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->gluoncv) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (54.2.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core->gluoncv) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core->gluoncv) (0.16.0)\n",
            "Collecting bcrypt>=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.2MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 49.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.55\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/b0/d382d472989ce23bee0a36f8bd0c326d4694496611a7fe41fae51d71bf46/botocore-1.20.55-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 42.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/95/91a1b99e4ef46bb915167b04aa26aec74dad5356d13d487a90f7d22994ee/s3transfer-0.4.1-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.22.0->autogluon.core->gluoncv) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx->gluoncv) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core->gluoncv) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core->gluoncv) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core->gluoncv) (2.20)\n",
            "Building wheels for collected packages: ConfigSpace\n",
            "  Building wheel for ConfigSpace (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2879844 sha256=4c0ff2f026675397bde99265c232590193d561e5e43d8ca6c0df47af28aed8df\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/ea/40/d93931850f700427db0a84180829c709d30484c9475040c7bd\n",
            "Successfully built ConfigSpace\n",
            "\u001b[31mERROR: distributed 2021.4.0 has requirement dask>=2021.03.0, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.55 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: autogluon-core 0.1.0 has requirement scipy==1.5.4, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, distributed, bcrypt, cryptography, pynacl, paramiko, jmespath, botocore, s3transfer, boto3, ConfigSpace, autogluon.core, tensorboardx, decord, yacs, portalocker, autocfg, gluoncv\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "Successfully installed ConfigSpace-0.4.18 autocfg-0.0.8 autogluon.core-0.1.0 bcrypt-3.2.0 boto3-1.17.55 botocore-1.20.55 cloudpickle-1.6.0 cryptography-3.4.7 decord-0.5.2 distributed-2021.4.0 gluoncv-0.10.1.post0 jmespath-0.10.0 paramiko-2.7.2 portalocker-2.3.0 pynacl-1.4.0 s3transfer-0.4.1 tensorboardx-2.2 yacs-0.1.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/4f/2e51ae21a0361db5363915f042f872a7d4ca456094a0b9e0f967bdcf7729/mxnet_cu101-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (356.7MB)\n",
            "\u001b[K     |████████████████████████████████| 356.7MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Installing collected packages: mxnet-cu101\n",
            "Successfully installed mxnet-cu101-1.8.0.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8vzZ0gfp0qZ"
      },
      "source": [
        "**2. Changing the working directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kFvgyGvgDmY",
        "outputId": "628961ec-13f8-46c4-b8f0-c4c1615a8b49"
      },
      "source": [
        "# Don't forget to mount the google drive\n",
        "%cd drive/MyDrive/Colab\\ Notebooks"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlk4oSsdhGaH"
      },
      "source": [
        "**3. Importing the requirments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gEmw5sMg3LB"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import argparse, time, logging, os, sys, math\n",
        "\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "import gluoncv as gcv\n",
        "from mxnet import gluon, nd, init, context\n",
        "from mxnet import autograd as ag\n",
        "from mxnet.gluon import nn\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "\n",
        "from gluoncv.data.transforms import video\n",
        "from gluoncv.data import VideoClsCustom\n",
        "from gluoncv.model_zoo import get_model\n",
        "from gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Os4bD85xROi"
      },
      "source": [
        "**4. Setting and loading the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDwVWE54gkPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e613140-d9f9-4ed5-dfd2-f35e6f579ccc"
      },
      "source": [
        "num_gpus = 1\n",
        "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
        "# transform_train = video.VideoGroupTrainTransform(size=(224, 224), scale_ratios=[1.0, 0.8], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    # Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\n",
        "    # {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\n",
        "    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n",
        "    # Randomly flip the video frames horizontally\n",
        "    video.VideoRandomHorizontalFlip(),\n",
        "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
        "    # and map values from [0, 255] to [0,1]\n",
        "    video.VideoToTensor(),\n",
        "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
        "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "per_device_batch_size = 5\n",
        "num_workers = 0\n",
        "batch_size = per_device_batch_size * num_gpus\n",
        "\n",
        "train_dataset = VideoClsCustom(root=os.path.expanduser('DataSet/train/'),\n",
        "                               setting=os.path.expanduser('DataSet/train/train.txt'),\n",
        "                               train=True,\n",
        "                               new_length=64,\n",
        "                               new_step=2,\n",
        "                               video_loader=True,\n",
        "                               use_decord=True,\n",
        "                               transform=transform_train)\n",
        "\n",
        "print('Load %d training samples.' % len(train_dataset))\n",
        "train_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                   shuffle=True, num_workers=num_workers)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load 294 training samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVPlnv3_yFyV"
      },
      "source": [
        "**5. Loading the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ggba8O6k-wA",
        "outputId": "80cc05a4-1d76-474e-dfe7-94544d5dcddb"
      },
      "source": [
        "net = get_model(name='i3d_resnet50_v1_custom', nclass=6)\n",
        "net.collect_params().reset_ctx(ctx)\n",
        "print(net)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/models/resnet50_v1b-0ecdba34.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet50_v1b-0ecdba34.zip...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 55344/55344 [00:01<00:00, 42409.94KB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conv0_weight is done with shape:  (64, 3, 5, 7, 7)\n",
            "batchnorm0_gamma is done with shape:  (64,)\n",
            "batchnorm0_beta is done with shape:  (64,)\n",
            "batchnorm0_running_mean is done with shape:  (64,)\n",
            "batchnorm0_running_var is done with shape:  (64,)\n",
            "layer1_0_conv0_weight is done with shape:  (64, 64, 3, 1, 1)\n",
            "layer1_0_batchnorm0_gamma is done with shape:  (64,)\n",
            "layer1_0_batchnorm0_beta is done with shape:  (64,)\n",
            "layer1_0_batchnorm0_running_mean is done with shape:  (64,)\n",
            "layer1_0_batchnorm0_running_var is done with shape:  (64,)\n",
            "layer1_0_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
            "layer1_0_batchnorm1_gamma is done with shape:  (64,)\n",
            "layer1_0_batchnorm1_beta is done with shape:  (64,)\n",
            "layer1_0_batchnorm1_running_mean is done with shape:  (64,)\n",
            "layer1_0_batchnorm1_running_var is done with shape:  (64,)\n",
            "layer1_0_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
            "layer1_0_batchnorm2_gamma is done with shape:  (256,)\n",
            "layer1_0_batchnorm2_beta is done with shape:  (256,)\n",
            "layer1_0_batchnorm2_running_mean is done with shape:  (256,)\n",
            "layer1_0_batchnorm2_running_var is done with shape:  (256,)\n",
            "layer1_downsample_conv0_weight is done with shape:  (256, 64, 1, 1, 1)\n",
            "layer1_downsample_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer1_downsample_batchnorm0_beta is done with shape:  (256,)\n",
            "layer1_downsample_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer1_downsample_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer1_1_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
            "layer1_1_batchnorm0_gamma is done with shape:  (64,)\n",
            "layer1_1_batchnorm0_beta is done with shape:  (64,)\n",
            "layer1_1_batchnorm0_running_mean is done with shape:  (64,)\n",
            "layer1_1_batchnorm0_running_var is done with shape:  (64,)\n",
            "layer1_1_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
            "layer1_1_batchnorm1_gamma is done with shape:  (64,)\n",
            "layer1_1_batchnorm1_beta is done with shape:  (64,)\n",
            "layer1_1_batchnorm1_running_mean is done with shape:  (64,)\n",
            "layer1_1_batchnorm1_running_var is done with shape:  (64,)\n",
            "layer1_1_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
            "layer1_1_batchnorm2_gamma is done with shape:  (256,)\n",
            "layer1_1_batchnorm2_beta is done with shape:  (256,)\n",
            "layer1_1_batchnorm2_running_mean is done with shape:  (256,)\n",
            "layer1_1_batchnorm2_running_var is done with shape:  (256,)\n",
            "layer1_2_conv0_weight is done with shape:  (64, 256, 3, 1, 1)\n",
            "layer1_2_batchnorm0_gamma is done with shape:  (64,)\n",
            "layer1_2_batchnorm0_beta is done with shape:  (64,)\n",
            "layer1_2_batchnorm0_running_mean is done with shape:  (64,)\n",
            "layer1_2_batchnorm0_running_var is done with shape:  (64,)\n",
            "layer1_2_conv1_weight is done with shape:  (64, 64, 1, 3, 3)\n",
            "layer1_2_batchnorm1_gamma is done with shape:  (64,)\n",
            "layer1_2_batchnorm1_beta is done with shape:  (64,)\n",
            "layer1_2_batchnorm1_running_mean is done with shape:  (64,)\n",
            "layer1_2_batchnorm1_running_var is done with shape:  (64,)\n",
            "layer1_2_conv2_weight is done with shape:  (256, 64, 1, 1, 1)\n",
            "layer1_2_batchnorm2_gamma is done with shape:  (256,)\n",
            "layer1_2_batchnorm2_beta is done with shape:  (256,)\n",
            "layer1_2_batchnorm2_running_mean is done with shape:  (256,)\n",
            "layer1_2_batchnorm2_running_var is done with shape:  (256,)\n",
            "layer2_0_conv0_weight is done with shape:  (128, 256, 3, 1, 1)\n",
            "layer2_0_batchnorm0_gamma is done with shape:  (128,)\n",
            "layer2_0_batchnorm0_beta is done with shape:  (128,)\n",
            "layer2_0_batchnorm0_running_mean is done with shape:  (128,)\n",
            "layer2_0_batchnorm0_running_var is done with shape:  (128,)\n",
            "layer2_0_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
            "layer2_0_batchnorm1_gamma is done with shape:  (128,)\n",
            "layer2_0_batchnorm1_beta is done with shape:  (128,)\n",
            "layer2_0_batchnorm1_running_mean is done with shape:  (128,)\n",
            "layer2_0_batchnorm1_running_var is done with shape:  (128,)\n",
            "layer2_0_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
            "layer2_0_batchnorm2_gamma is done with shape:  (512,)\n",
            "layer2_0_batchnorm2_beta is done with shape:  (512,)\n",
            "layer2_0_batchnorm2_running_mean is done with shape:  (512,)\n",
            "layer2_0_batchnorm2_running_var is done with shape:  (512,)\n",
            "layer2_downsample_conv0_weight is done with shape:  (512, 256, 1, 1, 1)\n",
            "layer2_downsample_batchnorm0_gamma is done with shape:  (512,)\n",
            "layer2_downsample_batchnorm0_beta is done with shape:  (512,)\n",
            "layer2_downsample_batchnorm0_running_mean is done with shape:  (512,)\n",
            "layer2_downsample_batchnorm0_running_var is done with shape:  (512,)\n",
            "layer2_1_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
            "layer2_1_batchnorm0_gamma is done with shape:  (128,)\n",
            "layer2_1_batchnorm0_beta is done with shape:  (128,)\n",
            "layer2_1_batchnorm0_running_mean is done with shape:  (128,)\n",
            "layer2_1_batchnorm0_running_var is done with shape:  (128,)\n",
            "layer2_1_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
            "layer2_1_batchnorm1_gamma is done with shape:  (128,)\n",
            "layer2_1_batchnorm1_beta is done with shape:  (128,)\n",
            "layer2_1_batchnorm1_running_mean is done with shape:  (128,)\n",
            "layer2_1_batchnorm1_running_var is done with shape:  (128,)\n",
            "layer2_1_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
            "layer2_1_batchnorm2_gamma is done with shape:  (512,)\n",
            "layer2_1_batchnorm2_beta is done with shape:  (512,)\n",
            "layer2_1_batchnorm2_running_mean is done with shape:  (512,)\n",
            "layer2_1_batchnorm2_running_var is done with shape:  (512,)\n",
            "layer2_2_conv0_weight is done with shape:  (128, 512, 3, 1, 1)\n",
            "layer2_2_batchnorm0_gamma is done with shape:  (128,)\n",
            "layer2_2_batchnorm0_beta is done with shape:  (128,)\n",
            "layer2_2_batchnorm0_running_mean is done with shape:  (128,)\n",
            "layer2_2_batchnorm0_running_var is done with shape:  (128,)\n",
            "layer2_2_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
            "layer2_2_batchnorm1_gamma is done with shape:  (128,)\n",
            "layer2_2_batchnorm1_beta is done with shape:  (128,)\n",
            "layer2_2_batchnorm1_running_mean is done with shape:  (128,)\n",
            "layer2_2_batchnorm1_running_var is done with shape:  (128,)\n",
            "layer2_2_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
            "layer2_2_batchnorm2_gamma is done with shape:  (512,)\n",
            "layer2_2_batchnorm2_beta is done with shape:  (512,)\n",
            "layer2_2_batchnorm2_running_mean is done with shape:  (512,)\n",
            "layer2_2_batchnorm2_running_var is done with shape:  (512,)\n",
            "layer2_3_conv0_weight is done with shape:  (128, 512, 1, 1, 1)\n",
            "layer2_3_batchnorm0_gamma is done with shape:  (128,)\n",
            "layer2_3_batchnorm0_beta is done with shape:  (128,)\n",
            "layer2_3_batchnorm0_running_mean is done with shape:  (128,)\n",
            "layer2_3_batchnorm0_running_var is done with shape:  (128,)\n",
            "layer2_3_conv1_weight is done with shape:  (128, 128, 1, 3, 3)\n",
            "layer2_3_batchnorm1_gamma is done with shape:  (128,)\n",
            "layer2_3_batchnorm1_beta is done with shape:  (128,)\n",
            "layer2_3_batchnorm1_running_mean is done with shape:  (128,)\n",
            "layer2_3_batchnorm1_running_var is done with shape:  (128,)\n",
            "layer2_3_conv2_weight is done with shape:  (512, 128, 1, 1, 1)\n",
            "layer2_3_batchnorm2_gamma is done with shape:  (512,)\n",
            "layer2_3_batchnorm2_beta is done with shape:  (512,)\n",
            "layer2_3_batchnorm2_running_mean is done with shape:  (512,)\n",
            "layer2_3_batchnorm2_running_var is done with shape:  (512,)\n",
            "layer3_0_conv0_weight is done with shape:  (256, 512, 3, 1, 1)\n",
            "layer3_0_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_0_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_0_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_0_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_0_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_0_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_0_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_0_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_0_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_0_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_0_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_0_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_0_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_0_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer3_downsample_conv0_weight is done with shape:  (1024, 512, 1, 1, 1)\n",
            "layer3_downsample_batchnorm0_gamma is done with shape:  (1024,)\n",
            "layer3_downsample_batchnorm0_beta is done with shape:  (1024,)\n",
            "layer3_downsample_batchnorm0_running_mean is done with shape:  (1024,)\n",
            "layer3_downsample_batchnorm0_running_var is done with shape:  (1024,)\n",
            "layer3_1_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
            "layer3_1_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_1_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_1_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_1_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_1_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_1_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_1_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_1_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_1_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_1_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_1_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_1_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_1_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_1_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer3_2_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
            "layer3_2_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_2_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_2_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_2_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_2_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_2_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_2_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_2_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_2_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_2_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_2_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_2_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_2_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_2_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer3_3_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
            "layer3_3_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_3_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_3_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_3_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_3_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_3_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_3_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_3_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_3_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_3_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_3_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_3_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_3_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_3_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer3_4_conv0_weight is done with shape:  (256, 1024, 3, 1, 1)\n",
            "layer3_4_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_4_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_4_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_4_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_4_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_4_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_4_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_4_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_4_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_4_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_4_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_4_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_4_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_4_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer3_5_conv0_weight is done with shape:  (256, 1024, 1, 1, 1)\n",
            "layer3_5_batchnorm0_gamma is done with shape:  (256,)\n",
            "layer3_5_batchnorm0_beta is done with shape:  (256,)\n",
            "layer3_5_batchnorm0_running_mean is done with shape:  (256,)\n",
            "layer3_5_batchnorm0_running_var is done with shape:  (256,)\n",
            "layer3_5_conv1_weight is done with shape:  (256, 256, 1, 3, 3)\n",
            "layer3_5_batchnorm1_gamma is done with shape:  (256,)\n",
            "layer3_5_batchnorm1_beta is done with shape:  (256,)\n",
            "layer3_5_batchnorm1_running_mean is done with shape:  (256,)\n",
            "layer3_5_batchnorm1_running_var is done with shape:  (256,)\n",
            "layer3_5_conv2_weight is done with shape:  (1024, 256, 1, 1, 1)\n",
            "layer3_5_batchnorm2_gamma is done with shape:  (1024,)\n",
            "layer3_5_batchnorm2_beta is done with shape:  (1024,)\n",
            "layer3_5_batchnorm2_running_mean is done with shape:  (1024,)\n",
            "layer3_5_batchnorm2_running_var is done with shape:  (1024,)\n",
            "layer4_0_conv0_weight is done with shape:  (512, 1024, 1, 1, 1)\n",
            "layer4_0_batchnorm0_gamma is done with shape:  (512,)\n",
            "layer4_0_batchnorm0_beta is done with shape:  (512,)\n",
            "layer4_0_batchnorm0_running_mean is done with shape:  (512,)\n",
            "layer4_0_batchnorm0_running_var is done with shape:  (512,)\n",
            "layer4_0_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
            "layer4_0_batchnorm1_gamma is done with shape:  (512,)\n",
            "layer4_0_batchnorm1_beta is done with shape:  (512,)\n",
            "layer4_0_batchnorm1_running_mean is done with shape:  (512,)\n",
            "layer4_0_batchnorm1_running_var is done with shape:  (512,)\n",
            "layer4_0_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
            "layer4_0_batchnorm2_gamma is done with shape:  (2048,)\n",
            "layer4_0_batchnorm2_beta is done with shape:  (2048,)\n",
            "layer4_0_batchnorm2_running_mean is done with shape:  (2048,)\n",
            "layer4_0_batchnorm2_running_var is done with shape:  (2048,)\n",
            "layer4_downsample_conv0_weight is done with shape:  (2048, 1024, 1, 1, 1)\n",
            "layer4_downsample_batchnorm0_gamma is done with shape:  (2048,)\n",
            "layer4_downsample_batchnorm0_beta is done with shape:  (2048,)\n",
            "layer4_downsample_batchnorm0_running_mean is done with shape:  (2048,)\n",
            "layer4_downsample_batchnorm0_running_var is done with shape:  (2048,)\n",
            "layer4_1_conv0_weight is done with shape:  (512, 2048, 3, 1, 1)\n",
            "layer4_1_batchnorm0_gamma is done with shape:  (512,)\n",
            "layer4_1_batchnorm0_beta is done with shape:  (512,)\n",
            "layer4_1_batchnorm0_running_mean is done with shape:  (512,)\n",
            "layer4_1_batchnorm0_running_var is done with shape:  (512,)\n",
            "layer4_1_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
            "layer4_1_batchnorm1_gamma is done with shape:  (512,)\n",
            "layer4_1_batchnorm1_beta is done with shape:  (512,)\n",
            "layer4_1_batchnorm1_running_mean is done with shape:  (512,)\n",
            "layer4_1_batchnorm1_running_var is done with shape:  (512,)\n",
            "layer4_1_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
            "layer4_1_batchnorm2_gamma is done with shape:  (2048,)\n",
            "layer4_1_batchnorm2_beta is done with shape:  (2048,)\n",
            "layer4_1_batchnorm2_running_mean is done with shape:  (2048,)\n",
            "layer4_1_batchnorm2_running_var is done with shape:  (2048,)\n",
            "layer4_2_conv0_weight is done with shape:  (512, 2048, 1, 1, 1)\n",
            "layer4_2_batchnorm0_gamma is done with shape:  (512,)\n",
            "layer4_2_batchnorm0_beta is done with shape:  (512,)\n",
            "layer4_2_batchnorm0_running_mean is done with shape:  (512,)\n",
            "layer4_2_batchnorm0_running_var is done with shape:  (512,)\n",
            "layer4_2_conv1_weight is done with shape:  (512, 512, 1, 3, 3)\n",
            "layer4_2_batchnorm1_gamma is done with shape:  (512,)\n",
            "layer4_2_batchnorm1_beta is done with shape:  (512,)\n",
            "layer4_2_batchnorm1_running_mean is done with shape:  (512,)\n",
            "layer4_2_batchnorm1_running_var is done with shape:  (512,)\n",
            "layer4_2_conv2_weight is done with shape:  (2048, 512, 1, 1, 1)\n",
            "layer4_2_batchnorm2_gamma is done with shape:  (2048,)\n",
            "layer4_2_batchnorm2_beta is done with shape:  (2048,)\n",
            "layer4_2_batchnorm2_running_mean is done with shape:  (2048,)\n",
            "layer4_2_batchnorm2_running_var is done with shape:  (2048,)\n",
            "dense0_weight is skipped with shape:  (6, 2048)\n",
            "dense0_bias is skipped with shape:  (6,)\n",
            "Downloading /root/.mxnet/models/i3d_resnet50_v1_kinetics400-568a722e.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/i3d_resnet50_v1_kinetics400-568a722e.zip...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 208483/208483 [00:03<00:00, 55527.50KB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I3D_ResNetV1(\n",
            "  (first_stage): HybridSequential(\n",
            "    (0): Conv3D(3 -> 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
            "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "    (2): Activation(relu)\n",
            "    (3): MaxPool3D(size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
            "  )\n",
            "  (pool2): MaxPool3D(size=(2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCDHW)\n",
            "  (res_layers): HybridSequential(\n",
            "    (0): HybridSequential(\n",
            "      (0): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        )\n",
            "        (conv1): Conv3D(64 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (relu): Activation(relu)\n",
            "        (downsample): HybridSequential(\n",
            "          (0): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        )\n",
            "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        )\n",
            "        (conv1): Conv3D(256 -> 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(64 -> 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
            "        (conv3): Conv3D(64 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "    )\n",
            "    (1): HybridSequential(\n",
            "      (0): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        )\n",
            "        (conv1): Conv3D(256 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (relu): Activation(relu)\n",
            "        (downsample): HybridSequential(\n",
            "          (0): Conv3D(256 -> 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        )\n",
            "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        )\n",
            "        (conv1): Conv3D(512 -> 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        )\n",
            "        (conv1): Conv3D(512 -> 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(128 -> 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
            "        (conv3): Conv3D(128 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "    )\n",
            "    (2): HybridSequential(\n",
            "      (0): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(512 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "        (downsample): HybridSequential(\n",
            "          (0): Conv3D(512 -> 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(256 -> 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
            "        (conv3): Conv3D(256 -> 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "    )\n",
            "    (3): HybridSequential(\n",
            "      (0): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        )\n",
            "        (conv1): Conv3D(1024 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        (relu): Activation(relu)\n",
            "        (downsample): HybridSequential(\n",
            "          (0): Conv3D(1024 -> 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        )\n",
            "        (conv1): Conv3D(2048 -> 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
            "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (bottleneck): HybridSequential(\n",
            "          (0): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (2): Activation(relu)\n",
            "          (3): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "          (5): Activation(relu)\n",
            "          (6): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "          (7): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        )\n",
            "        (conv1): Conv3D(2048 -> 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (conv2): Conv3D(512 -> 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
            "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
            "        (conv3): Conv3D(512 -> 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
            "        (relu): Activation(relu)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (st_avg): GlobalAvgPool3D(size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCDHW)\n",
            "  (head): HybridSequential(\n",
            "    (0): Dropout(p = 0.8, axes=())\n",
            "    (1): Dense(2048 -> 6, linear)\n",
            "  )\n",
            "  (fc): Dense(2048 -> 6, linear)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZ0sEVlqlmc"
      },
      "source": [
        "**6. Setting up the hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8R9iZsPlDm1"
      },
      "source": [
        "lr_decay = 0.1\n",
        "# Epochs where learning rate decays\n",
        "lr_decay_epoch = [40, 80, 100]\n",
        "\n",
        "# Stochastic gradient descent\n",
        "optimizer = 'sgd'\n",
        "# Set parameters\n",
        "optimizer_params = {'learning_rate': 0.001, 'wd': 0.0001, 'momentum': 0.9}\n",
        "\n",
        "# Define our trainer for net\n",
        "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxK6g2cklGTw"
      },
      "source": [
        "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWnitvEFlISM"
      },
      "source": [
        "train_metric = mx.metric.Accuracy()\n",
        "train_history = TrainingHistory(['training-acc'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2wlE2vjq4xR"
      },
      "source": [
        "**7. Trainning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pR9dN5dlJwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "34a5c1e1-4d7d-4d5c-e850-c692975fd339"
      },
      "source": [
        "epochs = 10\n",
        "lr_decay_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    tic = time.time()\n",
        "    train_metric.reset()\n",
        "    train_loss = 0\n",
        "\n",
        "    # Learning rate decay\n",
        "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
        "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
        "        lr_decay_count += 1\n",
        "\n",
        "    # Loop through each batch of training data\n",
        "    for i, batch in enumerate(train_data):\n",
        "        # Extract data and label\n",
        "        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
        "\n",
        "        # AutoGrad\n",
        "        with ag.record():\n",
        "            output = []\n",
        "            for _, X in enumerate(data):\n",
        "                X = X.reshape((-1,) + X.shape[2:])\n",
        "                pred = net(X)\n",
        "                output.append(pred)\n",
        "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
        "\n",
        "        # Backpropagation\n",
        "        for l in loss:\n",
        "            l.backward()\n",
        "\n",
        "        # Optimize\n",
        "        trainer.step(batch_size)\n",
        "\n",
        "        # Update metrics\n",
        "        train_loss += sum([l.mean().asscalar() for l in loss])\n",
        "        train_metric.update(label, output)\n",
        "\n",
        "        if i == 100:\n",
        "            break\n",
        "\n",
        "    name, acc = train_metric.get()\n",
        "\n",
        "    # Update history and print metrics\n",
        "    train_history.update([acc])\n",
        "    print('[Epoch %d] accuracy= %f loss= %f time: %f' %\n",
        "        (epoch, acc, train_loss / (i+1), time.time()-tic))\n",
        "\n",
        "# We can plot the metric scores with:\n",
        "train_history.plot()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0] accuracy= 0.241497 loss= 1.714057 time: 478.775431\n",
            "[Epoch 1] accuracy= 0.588435 loss= 1.187268 time: 279.396857\n",
            "[Epoch 2] accuracy= 0.840136 loss= 0.604421 time: 279.135364\n",
            "[Epoch 3] accuracy= 0.884354 loss= 0.405462 time: 279.117466\n",
            "[Epoch 4] accuracy= 0.911565 loss= 0.307114 time: 278.571351\n",
            "[Epoch 5] accuracy= 0.938776 loss= 0.224519 time: 279.049941\n",
            "[Epoch 6] accuracy= 0.948980 loss= 0.180747 time: 278.303482\n",
            "[Epoch 7] accuracy= 0.945578 loss= 0.160579 time: 278.267199\n",
            "[Epoch 8] accuracy= 0.959184 loss= 0.167270 time: 279.515679\n",
            "[Epoch 9] accuracy= 0.962585 loss= 0.111137 time: 279.436798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iudoD0AerBMk"
      },
      "source": [
        "**8. Saving the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooqs46lM9GTi"
      },
      "source": [
        "net.save_parameters('Model/i3d_resnet50_v1.params')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzWf7zKbrH3V"
      },
      "source": [
        "**9. Running my infrence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvUn4P_SUhn"
      },
      "source": [
        "# Map\n",
        "CLASS_MAP = {\n",
        "    0: \"Opaque\",\n",
        "    1: \"Red\",\n",
        "    2: \"Green\",\n",
        "    3: \"Yellow\",\n",
        "    4: \"Bright\",\n",
        "    5: \"Light-blue\"\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T24lM2Xf3td-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3620c73-6b89-4c7e-ef73-13449ddec3c0"
      },
      "source": [
        "from gluoncv.utils.filesystem import try_import_decord\n",
        "decord = try_import_decord()\n",
        "\n",
        "video_fname = 'DataSet/test/006_001_001.mp4'\n",
        "vr = decord.VideoReader(video_fname)\n",
        "frame_id_list = range(0, 64, 2)\n",
        "video_data = vr.get_batch(frame_id_list).asnumpy()\n",
        "clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\n",
        "\n",
        "# transform_fn = video.VideoGroupValTransform(size=(224, 224), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_fn = transforms.Compose([\n",
        "    # Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\n",
        "    # {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\n",
        "    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n",
        "    # Randomly flip the video frames horizontally\n",
        "    video.VideoRandomHorizontalFlip(),\n",
        "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
        "    # and map values from [0, 255] to [0,1]\n",
        "    video.VideoToTensor(),\n",
        "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
        "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "clip_input = transform_fn(clip_input)\n",
        "clip_input = np.stack(clip_input, axis=0)\n",
        "clip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\n",
        "clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\n",
        "print('Video data is readed and preprocessed.')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video data is readed and preprocessed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73JMNAs-4z8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68c3a67-1c00-4086-9fb2-f1579ba8c81b"
      },
      "source": [
        "# Running the prediction\n",
        "pred = net(nd.array(clip_input,  ctx = mx.gpu(0)))\n",
        "topK = 5\n",
        "ind = nd.topk(pred, k=topK)[0].astype('int')\n",
        "print('The input video clip is classified to be')\n",
        "for i in range(topK):\n",
        "    print('\\t[%s], with probability %.3f.'%\n",
        "          (CLASS_MAP[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input video clip is classified to be\n",
            "\t[Light-blue], with probability 0.999.\n",
            "\t[Opaque], with probability 0.000.\n",
            "\t[Green], with probability 0.000.\n",
            "\t[Yellow], with probability 0.000.\n",
            "\t[Bright], with probability 0.000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXFEaylkxN4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}